{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb3ae1e-d68c-4012-9e16-8167d14cb3fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Dimension Table: Station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b71170-c680-4399-969b-e47725d56565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Station Dimension Table: dim_station\n",
    "dim_station = spark.read.table(\"staging_station\")\n",
    "# Save as a table in Delta\n",
    "dim_station.write.format('delta').mode('overwrite').saveAsTable('dim_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b1d2ba-d872-4db2-89e1-72b5748bd2ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Dimension Table: Rider**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc07b3ff-62f9-4748-8055-cdac9e5ee1d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, year, to_date\n",
    "# Load the staging_rider table\n",
    "df_rider = spark.read.table(\"staging_rider\")\n",
    "# Transform the data for dim_rider\n",
    "dim_rider = df_rider.select(\n",
    "    col(\"rider_id\"),\n",
    "    col(\"first\"),\n",
    "    col(\"last\"),\n",
    "    to_date(col(\"birthday\"), \"yyyy-MM-dd\").alias(\"birthday\"),\n",
    "    to_date(col(\"account_start_date\"), \"yyyy-MM-dd\").alias(\"account_start_date\"),\n",
    "    (year(col(\"account_start_date\")) - year(col(\"birthday\"))).alias(\"age_at_account_start\"),\n",
    "    when(col(\"is_member\") == 1, \"Member\").otherwise(\"Casual\").alias(\"membership_status\")\n",
    ")\n",
    "\n",
    "# Write the data to the dim_rider table to a Delta table\n",
    "dim_rider.write.format('delta').mode('overwrite').saveAsTable('dim_rider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd3e321-eb0a-4f5f-8ca9-66d77a9e2b9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Dimension Table: Date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1311149a-6fb4-471b-8f4a-e0ddd974fdc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, date_add, date_sub, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load the staging_payment table\n",
    "df_payment = spark.read.table(\"staging_payment\")\n",
    "# Drop any rows where date is null\n",
    "df_payment = df_payment.dropna(subset=[\"date\"])\n",
    "# Extract the minimum and maximum dates\n",
    "min_date = df_payment.agg({\"date\": \"min\"}).collect()[0][0]\n",
    "max_date = df_payment.agg({\"date\": \"max\"}).collect()[0][0]\n",
    "# Generate a sequence of dates from min_date to max_date\n",
    "date_range_df = spark.range(0, (max_date - min_date).days + 1).selectExpr(f\"date_add('{min_date}', CAST(id AS INT)) as date\")\n",
    "# Create the dim_date DataFrame with date_id starting from 1 to n\n",
    "dim_date = date_range_df.withColumn(\"date_id\", row_number().over(Window.orderBy(\"date\"))) \\\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"month\", date_format(col(\"date\"), \"MM\").cast(\"int\")) \\\n",
    "    .withColumn(\"quarter\", date_format(col(\"date\"), \"Q\").cast(\"int\")) \\\n",
    "    .withColumn(\"year\", date_format(col(\"date\"), \"yyyy\").cast(\"int\"))\n",
    "\n",
    "# Write the data to the dim_date table to a Delta table\n",
    "dim_date.write.format('delta').mode('overwrite').saveAsTable('dim_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbdee90c-d6cf-477c-a515-f88ff78b4e0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Fact Table: Payment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeaceca-2ccf-40b6-affd-02072103e3b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the staging_payment table\n",
    "df_payment = spark.read.table(\"staging_payment\")\n",
    "# Convert the payment date to a date type\n",
    "df_payment = df_payment.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fd5102-5c7f-4482-876e-c192ec942556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join staging_payment with dim_date to get the date_id\n",
    "fact_payment_df = df_payment.join(\n",
    "    dim_date,\n",
    "    df_payment[\"date\"] == dim_date[\"date\"],\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"payment_id\"),\n",
    "    col(\"date_id\").alias(\"payment_date_id\"),\n",
    "    col(\"amount\"),\n",
    "    col(\"rider_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b4e0a5-c036-4e6d-b5cd-1ac0bffbb40b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join fact_payment_df with dim_rider\n",
    "fact_payment = fact_payment_df.join(\n",
    "    dim_rider,\n",
    "    fact_payment_df[\"rider_id\"] == dim_rider[\"rider_id\"],\n",
    "    \"inner\"\n",
    ").select(\n",
    "    fact_payment_df[\"payment_id\"],\n",
    "    fact_payment_df[\"payment_date_id\"],\n",
    "    fact_payment_df[\"amount\"],\n",
    "    fact_payment_df[\"rider_id\"]  # Explicitly selecting rider_id from fact_payment_df\n",
    ")\n",
    "\n",
    "\n",
    "# Write the fact_payment DataFrame to a Delta table\n",
    "fact_payment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_payment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d62d1cb-6c44-4bed-900c-005ba3e654c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Fact Table: Trip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c39984-607b-4449-a9b5-337cf33af5e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------+--------------+------------+-----------------+---------+\n|         trip_id|rider_id|start_station_id|end_station_id|trip_date_id|         duration|rider_age|\n+----------------+--------+----------------+--------------+------------+-----------------+---------+\n|222BB8E5059252D7|   34062|    KA1503000064|         13021|        3055|             18.6|       30|\n|1826E16CB5486018|    5342|    TA1306000010|         13021|        3063|5.266666666666667|       26|\n|3D9B6A0A5330B04D|    3714|    TA1305000030|         13021|        3060|5.333333333333333|       26|\n+----------------+--------+----------------+--------------+------------+-----------------+---------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_date, expr, substring, year\n",
    "\n",
    "# Load the staging_trip table\n",
    "df_trip = spark.read.table(\"staging_trip\")\n",
    "\n",
    "# Truncate datetime strings to 'YYYY-MM-DD HH:MM:SS' format and convert to timestamp\n",
    "df_trip = df_trip.withColumn(\"started_at\", substring(\"started_at\", 1, 19).cast(\"timestamp\")) \\\n",
    "                 .withColumn(\"ended_at\", substring(\"ended_at\", 1, 19).cast(\"timestamp\"))\n",
    "\n",
    "# Join staging_trip with dim_date to get the trip_date_id\n",
    "fact_trip_df = df_trip.join(\n",
    "    dim_date,\n",
    "    to_date(df_trip[\"started_at\"]) == dim_date[\"date\"],\n",
    "    \"inner\"\n",
    ").select(\n",
    "    \"trip_id\",\n",
    "    \"rider_id\",\n",
    "    \"start_station_id\",\n",
    "    \"end_station_id\",\n",
    "    \"started_at\",  # Include started_at for later age calculation\n",
    "    dim_date[\"date_id\"].alias(\"trip_date_id\"),\n",
    "    ((unix_timestamp(\"ended_at\") - unix_timestamp(\"started_at\")) / 60).alias(\"duration\")  # Duration in minutes\n",
    ")\n",
    "\n",
    "# Join with dim_rider to ensure the rider exists in dim_rider\n",
    "fact_trip = fact_trip_df.join(\n",
    "    dim_rider,\n",
    "    fact_trip_df[\"rider_id\"] == dim_rider[\"rider_id\"],\n",
    "    \"inner\"\n",
    ").select(\n",
    "    \"trip_id\",\n",
    "    fact_trip_df[\"rider_id\"],\n",
    "    \"start_station_id\",\n",
    "    \"end_station_id\",\n",
    "    \"trip_date_id\",\n",
    "    \"duration\",\n",
    "    (year(fact_trip_df[\"started_at\"]) - year(dim_rider[\"birthday\"])).alias(\"rider_age\")\n",
    ")\n",
    "\n",
    "# Show the first 3 records to verify the data\n",
    "fact_trip.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61ef65c-6cf2-4cfe-8743-b2fa94d6eca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the fact_trip DataFrame to a Delta table\n",
    "fact_trip.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0609634-f8aa-40ed-b8f6-d4ff04188c67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
