{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d02e2b6f-fb14-4ff3-9aa3-620275f4a04f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Task 3: Load data from Delta to create Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8c266d-2cac-4df5-80ef-f2f2ef9a5337",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create staging_payment table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE staging_payment\n",
    "    USING DELTA \n",
    "    AS\n",
    "    SELECT \n",
    "        CAST(payment_id AS BIGINT) AS payment_id,\n",
    "        CAST(date AS TIMESTAMP) AS date,\n",
    "        CAST(amount AS FLOAT) AS amount,\n",
    "        CAST(ride_id AS BIGINT) AS rider_id\n",
    "    FROM delta.`/bronze/payments`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35e4569-e2d4-44af-86cf-5e515a07470b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create staging_station table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE staging_station\n",
    "    USING DELTA \n",
    "    AS\n",
    "    SELECT \n",
    "        station_id AS station_id,\n",
    "        name AS name,\n",
    "        CAST(latitude AS FLOAT) AS latitude,\n",
    "        CAST(longitude AS FLOAT) AS longitude\n",
    "    FROM delta.`/bronze/stations`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59ac2e4-6b9a-40c1-b8c0-23120491d6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create staging_rider table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE staging_rider\n",
    "    USING DELTA \n",
    "    AS\n",
    "    SELECT \n",
    "        CAST(rider_id AS BIGINT) AS rider_id,\n",
    "        first AS first,\n",
    "        last AS last,\n",
    "        address AS address,\n",
    "        CAST(birthday AS TIMESTAMP) AS birthday,\n",
    "        CAST(account_start_date AS TIMESTAMP) AS account_start_date,\n",
    "        CAST(account_end_date AS TIMESTAMP) AS account_end_date,\n",
    "        CASE WHEN is_member = 'True' THEN TRUE ELSE FALSE END AS is_member\n",
    "    FROM delta.`/bronze/riders`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c75a752-49fd-456e-93e9-45e9e5ca2876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create staging_trip table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE staging_trip\n",
    "    USING DELTA \n",
    "    AS\n",
    "    SELECT \n",
    "        CAST(trip_id AS VARCHAR(50)) AS trip_id,\n",
    "        rideable_type AS rideable_type,\n",
    "        CAST(started_at AS TIMESTAMP) AS started_at,\n",
    "        CAST(ended_at AS TIMESTAMP) AS ended_at,\n",
    "        CAST(start_station_id AS BIGINT) AS start_station_id,\n",
    "        CAST(end_station_id AS BIGINT) AS end_station_id,\n",
    "        CAST(rider_id AS BIGINT) AS rider_id\n",
    "    FROM delta.`/bronze/trips`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7ce17b-f059-4aba-b0cb-af520a9bde21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Table(name='payments', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='riders', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='staging_payment', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='staging_rider', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='staging_station', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='staging_trip', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='stations', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='trips', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all tables in the database\n",
    "spark.catalog.listTables(spark.catalog.listDatabases()[0].name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
