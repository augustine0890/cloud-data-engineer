1. Amazon Athena Federated Query allows to directly run SQL queries across multiple data sources.
- For example, the marketing analyst to query data from Amazon DynamoDB, RDS, Redshift, S3 without the need to move or replicate data. It's cost-effective for a one-time analysis job, as it eliminates the overhead of data transfer and additional storage.
- The Athena's pay-per-query model and serverless nature also contribute to cost savings, especially for ad-hoc or one-time analytical tasks.
2. AWS CloudTrail is a service that provides a record of actions taken by a user, role, or an AWS service in AWS, enabling governance, compliance, operational auditing, and risk auditing or your AWS account.
- Trail with data event logging, the company can track and log all write operations (such as PUT, POST, and DELETE actions) made to their S3 bucket. These logs can be directed to another S3 bucket within the same region for storage and analysis.
3. AWS Data Pipeline is designed for data-driven workflows to process and move data between AWS compute and storage services.
- AppFlow also ensures secure data transfer with options for encryption and provides a user-friendly interface to manage data flows, making it accessible for users without deep technical expertise in coding or data integration.
4. SageMaker Experiments is a useful tool for organizing, tracking, and comparing iterations of machine learning models; it primarily focuses on experiment management rather the detailed lineage tracking of all artifacts throughout their lifecycle.
5. AWS Glue Workflows provides a managed ETL service that facilitates the preparation, transformation, and loading of data for analysis.
- It supports the orchestration of ETL jobs that can be triggered based on schedule or event conditions. The service is designed specifically for data integration tasks, offering a visual interface to define workflows, manage job dependencies, and monitor execution.
6. The most secure and recommended approach for managing access from Amazon EKS application to DynamoDB is to assign an IAM role to the EKS worker nodes with the necessary permissions to access DynamoDB and leverage the IAM roles for services accounts (IRSA) feature.
- With IRSA, app running on EKS can securely access AWS services without embedding AWS credentials within containers.
7. Initiating a retrieval job directly from Amazon Glacier allows you to retrieve the required data efficiently, albeit with a longer retrieval time compared to data stored in S3.
- Restoring the archived data from Glacier to S3 and querying it using Amazon Athena provides a cost-effective solution, as Glacier data retrieval fees can be higher for expedited retrieval. By utilizing Amazon Athena, you can query the data directly in S3 without incurring additional retrieval costs, making it a cost-effective approach to fulfill the request
8. CloudWatch Logs Insights can perform real-time log analysis using simple query language commands directly within the CloudWatch console.
- It provides interactive visualizations and querying capabilities, enabling the team to quickly identify patterns of application failures and devise appropriate remediation strategies.
9. Glue DataBrew is a visual data preparation tool that enables users to easily cleanse, normalize, and transform data without writing code.
- DataBrew can apply transformations such as masking or hashing to sensitive fields directly within the data engineering pipeline, ensuring that the dataset is compliant with financial regulations regarding data privacy.
- Amazon GuardDuty offers intelligent threat detection to monitor for suspicious activities, including potentially unauthorized access to data.
10. AWS Step Functions allows for the coordination of multiple AWS services into serverless workflows so that you can build and update apps quickly. By using Step Functions to manage the ETL process, you can check for the completeness of data from all providers before proceeding the load to Redshift.
11. Implement S3 Object Lock with a governance mode on the S3 bucket to prevent data from being deleted before 5-year retention period expires.
- Configure an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive for long-term storage. This policy allows for the data to be stored in a more cost-efficient manner after it is no longer actively queried, without violating the mandate to retain the data for 5 years.
12. DynamoDB can store and retrieve any amount of data, handling hundreds of terabytes without any issues. The service offers low latency on read and write operations and automatically manages scaling to adjust for capacity, ensuring your application can sustain its performance requirements without manual intervention.
13. Deploy an MSK cluster with at least three broker nodes spread across multiple AZs and enable auto-scaling based on CPU utilization.
- Use Kinesis Data Firehose for data ingestion, Apache Kafka Connect with the S3 sink connector for moving processed data to S3, and manage stream processing using Apache Spark.
14. DynamoDB Time-to-Live (TTL) feature to automatically delete items based on a specified expiry timestamp. DynamoDB automatically removes items that have exceeded their TTL, ensuring old data is deleted without the need for manual intervention or additional infrastructure.
15.  DPUs are a measure of processing power that consists of 4 vCPU and 16 GB of memory. By allocating more DPUs, the team can provide additional processing power to the ETL jobs, enabling faster data processing and reducing the overall job execution time.
16. The power of Amazon Redshift for analytics, providing a scalable and efficient way to analyze large datasets. AWS DMS facilitates the seamless and continuous replication of your operational data into Redshift, enabling complex analytics without affecting the source database's performance.
- Moving data into Amazon S3, a highly durable storage service, and using Athena, a serverless query service, to perform analytics. It separates the analytics workload from the operational database, thus preserving its performance while still allowing for insightful analysis of business data.