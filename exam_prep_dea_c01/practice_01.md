1. Amazon Athena Federated Query allows to directly run SQL queries across multiple data sources.
- For example, the marketing analyst to query data from Amazon DynamoDB, RDS, Redshift, S3 without the need to move or replicate data. It's cost-effective for a one-time analysis job, as it eliminates the overhead of data transfer and additional storage.
- The Athena's pay-per-query model and serverless nature also contribute to cost savings, especially for ad-hoc or one-time analytical tasks.
2. AWS CloudTrail is a service that provides a record of actions taken by a user, role, or an AWS service in AWS, enabling governance, compliance, operational auditing, and risk auditing or your AWS account.
- Trail with data event logging, the company can track and log all write operations (such as PUT, POST, and DELETE actions) made to their S3 bucket. These logs can be directed to another S3 bucket within the same region for storage and analysis.
3. AWS Data Pipeline is designed for data-driven workflows to process and move data between AWS compute and storage services.
- AppFlow also ensures secure data transfer with options for encryption and provides a user-friendly interface to manage data flows, making it accessible for users without deep technical expertise in coding or data integration.
4. SageMaker Experiments is a useful tool for organizing, tracking, and comparing iterations of machine learning models; it primarily focuses on experiment management rather the detailed lineage tracking of all artifacts throughout their lifecycle.
5. AWS Glue Workflows provides a managed ETL service that facilitates the preparation, transformation, and loading of data for analysis.
- It supports the orchestration of ETL jobs that can be triggered based on schedule or event conditions. The service is designed specifically for data integration tasks, offering a visual interface to define workflows, manage job dependencies, and monitor execution.
6. The most secure and recommended approach for managing access from Amazon EKS application to DynamoDB is to assign an IAM role to the EKS worker nodes with the necessary permissions to access DynamoDB and leverage the IAM roles for services accounts (IRSA) feature.
- With IRSA, app running on EKS can securely access AWS services without embedding AWS credentials within containers.
7. Initiating a retrieval job directly from Amazon Glacier allows you to retrieve the required data efficiently, albeit with a longer retrieval time compared to data stored in S3.
- Restoring the archived data from Glacier to S3 and querying it using Amazon Athena provides a cost-effective solution, as Glacier data retrieval fees can be higher for expedited retrieval. By utilizing Amazon Athena, you can query the data directly in S3 without incurring additional retrieval costs, making it a cost-effective approach to fulfill the request
8. CloudWatch Logs Insights can perform real-time log analysis using simple query language commands directly within the CloudWatch console.
- It provides interactive visualizations and querying capabilities, enabling the team to quickly identify patterns of application failures and devise appropriate remediation strategies.
9. Glue DataBrew is a visual data preparation tool that enables users to easily cleanse, normalize, and transform data without writing code.
- DataBrew can apply transformations such as masking or hashing to sensitive fields directly within the data engineering pipeline, ensuring that the dataset is compliant with financial regulations regarding data privacy.
- Amazon GuardDuty offers intelligent threat detection to monitor for suspicious activities, including potentially unauthorized access to data.
10. AWS Step Functions allows for the coordination of multiple AWS services into serverless workflows so that you can build and update apps quickly. By using Step Functions to manage the ETL process, you can check for the completeness of data from all providers before proceeding the load to Redshift.
11. Implement S3 Object Lock with a governance mode on the S3 bucket to prevent data from being deleted before 5-year retention period expires.
- Configure an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive for long-term storage. This policy allows for the data to be stored in a more cost-efficient manner after it is no longer actively queried, without violating the mandate to retain the data for 5 years.
12. DynamoDB can store and retrieve any amount of data, handling hundreds of terabytes without any issues. The service offers low latency on read and write operations and automatically manages scaling to adjust for capacity, ensuring your application can sustain its performance requirements without manual intervention.
13. Deploy an MSK cluster with at least three broker nodes spread across multiple AZs and enable auto-scaling based on CPU utilization.
- Use Kinesis Data Firehose for data ingestion, Apache Kafka Connect with the S3 sink connector for moving processed data to S3, and manage stream processing using Apache Spark.
14. DynamoDB Time-to-Live (TTL) feature to automatically delete items based on a specified expiry timestamp. DynamoDB automatically removes items that have exceeded their TTL, ensuring old data is deleted without the need for manual intervention or additional infrastructure.
15.  DPUs are a measure of processing power that consists of 4 vCPU and 16 GB of memory. By allocating more DPUs, the team can provide additional processing power to the ETL jobs, enabling faster data processing and reducing the overall job execution time.
16. The power of Amazon Redshift for analytics, providing a scalable and efficient way to analyze large datasets. AWS DMS facilitates the seamless and continuous replication of your operational data into Redshift, enabling complex analytics without affecting the source database's performance.
- Moving data into Amazon S3, a highly durable storage service, and using Athena, a serverless query service, to perform analytics. It separates the analytics workload from the operational database, thus preserving its performance while still allowing for insightful analysis of business data.
17. Amazon AppFlow includes native integration with SaaS applications that you can use daily for business operations. For example, Amazon AppFlow can integrate with Salesforce, SAP, Google Analytics, and Facebook Ads. With Amazon AppFlow, you can transfer data from any supported SaaS application in a few selections.
- An AWS Glue crawler connects to a data store source or target. Then, the AWS Glue crawler progresses through a prioritized list of classifiers to determine the schema for your data. Finally, the AWS Glue crawler creates metadata tables in the Data Catalog. A solution that points the crawler at an S3 data store will create the table definitions in the Data Catalog. After Amazon S3 ingests the data, you can use an AWS Glue crawler to populate the Data Catalog with tables. Then, you can start to consume the data by using SQL in Athena.
18. Migrate to EMR. Store all data in S3. Launch transient EMR clusters when jobs need to run.
- To store data in Amazon S3 is the most cost-effective and durable solution. Additionally, to use a transient cluster for batch jobs is a best practice. You do not need to spin up a long-running cluster for interactive usage. Transient clusters only run when jobs are submitted. Additionally, Amazon EMR can reuse existing jobs developed with Hadoop frameworks on premises.
- [Transient EMR clusters](https://github.com/aws/aws-emr-best-practices/blob/main/content/reliability/best_practices.md)
19. AWS Glue is a data integration service. You can use AWS Glue crawlers to automatically discover data and infer schema information. Data Catalog is a feature of AWS Glue used to catalog your data. AWS Glue DataBrew is used to clean and prepare data. AWS Glue Schema Registry is used to discover and control data schemas, and you can use it to maintain schema consistency. AWS Glue Schema Registry supports AVRO and JSON data format, and Protocol Buffers (Protobuf).
- [Data Catalog and crawlers in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html)
- [AWS Glue Schema Registry](https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html)
20.  Redshift Spectrum is a feature of Amazon Redshift. This feature is used to query data that is stored in Amazon S3. You can use the same SQL syntax and query editor as Amazon Redshift. You can join the data in Amazon S3 with the data in your Amazon Redshift cluster. Redshift Spectrum offers better performance, scalability, and compatibility for this use case and requires the least development effort because Redshift Spectrum is a feature in Amazon Redshift.
21. The Lambda function is timing out because of larger file size processing time.
- The scenario states that the entire file is not being completely processed for large files. Additionally, the scenario states that normal processing finishes within several minutes for smaller size files. Therefore, you can conclude that you might be breaching a configured timeout with the larger size files. Lambda supports timeouts up to 15 minutes.