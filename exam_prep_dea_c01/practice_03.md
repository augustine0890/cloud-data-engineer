1. AWS Glue crawler can scan the data in S3. Then, the crawler can infer the schema and create metadata in Data Catalog.
- You can run the crawler on a time-based schedule to detect any changes in the schema. The crawler can compare the previously generated metadata with the new data.
- Data Catalog is a persistent technical metadata store that provides a uniform repository that can store and find metadata to track the data.
- [Glue crawlers](https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html)
2. A solution that uses a Parquet or ORC column format is quick and compresses the data. You can use Parquet and ORC column formats as targets for database migration.
- Athena and QuickSight can read Parquet and ORC formats.
- Parquet and ORC were initially built for Apache Hadoop. Therefore, you can use Hadoop for data science programming with Spark.
- [S3 Data Lake](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/amazon-s3-data-lake-storage-platform.html)
3. By partitioning your data, you can restrict the amount of data scanned by each query. Therefore, you can improve performance and reduce cost.
- You can partition your data by any key. A common practice is to partition the data based on time, which leads to a multi-level partitioning schema. For example, a user with data that comes in every hour might decide to partition by year, month, date and hour.
- [Partition in Athena](https://docs.aws.amazon.com/athena/latest/ug/partitions.html)
4. AWS Macie provides automation to discover, log, and report sensitive data that is located in S3.
- [Discover sensitive data](https://docs.aws.amazon.com/macie/latest/user/data-classification.html)
- An AWS Glue job is a serverless job that simplifies the process of preparing and transforming data. AWS Glue jobs provide built-in transforms and data source connectors that accelerate extract, transform, and load (ETL) pipelines to ingest data into data lakes.
- The built-in Detect PII transform allows you to choose the PII entities to identify, the method used to scan the data, and how to handle the PII entities after they have been identified, such as masking or redaction.
- Using AWS Glue to ingest the job with the Detect PII transform means that data can be detected and masked without ever being exposed to unauthorized individuals.
- [Detect and process sensitive data](https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html)
5. Redshift is a fully managed data warehouse service. Redshift uses the PartiQL language with dotted notation and array subscript for path navigation when accessing nested data that is stored in SUPER columns.
- [Query semi-structured data](https://docs.aws.amazon.com/redshift/latest/dg/query-super.html)
6. With a gateway endpoint, you can access Amazon S3 from your VPC with nod additional cost. When you create an S3 gateway endpoint, the route table is updated with an S3 IP prefix list, which routes your VPC traffic directly to S3 from the newly created endpoint.
- AWS Glue jobs do not run in VPC by default. To access a VPC network, you need to first create an AWS Glue network connection. AWS Glue then sets up network interfaces in your VPC subnet by using this connection. This connection allows secure communication between AWS Glue and Amazon S3 by using a VPC network.
7. AWS Glue is a serverless data integration service. You can create an AWS Glue job that reads data from the RDS PostgreSQL table. By configuring the job to use AWS Glue bookmarks, you can ensure that only new data is loaded from the table.
8. Amazon Redshift data sharing gives you the ability to live-share data across Redshift clusters and serverless endpoints without the need to replicate the data. Amazon Redshift also supports cross-account data sharing.
- You can use cross-account data sharing managed with Lake Formation to grant the new Redshift cluster access to only the non-PII columns in the data analyst AWS account.
- [Datashare](https://docs.aws.amazon.com/redshift/latest/dg/what_is_datashare.html#lf_datashare_overview)
9. You can use Athena to directly query data in Amazon S3. Athena is serverless. Therefore, you do not need to select the infrastructure necessary to run the queries for the monthly process. You can use Athena tables and views to query a subset of the data. You can use Athena tables and views to combine multiple tables and datasets into one query.
- [Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html)
10. Athena is a query service that is used to query data in Amazon S3 by using standard SQL. When you partition data, the query engine scans less data for queries that filter based on the partitioned column. This step results in faster queries and lower query costs. This step is an important optimization technique in Athena.
- [Data Optimization](https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html#performance-tuning-data-optimization-techniques)
- When you compress files, it reduces the amount of data that Athena needs to scan. This step results in faster queries and lower query costs.
- [Athena compression](https://docs.aws.amazon.com/athena/latest/ug/compression-formats.html)
11. Kinesis Data Streams can capture the source data in near real time. You can use Lambda to perform quick transformations on the data. Then, Lambda can transfer the data to an S3 bucket.
- Kinesis Data Streams has retry functionality in the event of a processing error. You can configure Kinesis Data Firehose as a second consumer for Kinesis Data Streams. Kinesis Firehose can transfer the data directly to an S3 bucket.
12. A solution that directly queries ProductID will retrieve all reviews for that product sorted by the most recent because the sort key is set to ReviewTimestamp. A solution that queries the ProductID with a limit clause will retrieve the latest reviews for that specific product. This design will ensure that the table is optimized for the necessary access patterns. This design centers the primary table around the product.
13. You can use the groupSize parameter to group files that are being read from Amazon S3. This solution creates a larger partition in the AWS Glue job. The parameter gives you the ability to specify the target size of the group in bytes. The parameter is especially useful when reading a large number of small files in Amazon S3. A solution that uses grouping to create appropriately sized partitions will not increase the length of the job and might even make the job more performant.
- Defining a significantly large `groupSize` can result in an underutilized cluster. This solution is useful when your read directly from Amazon S3 and have a large number of small files because each ETL task can still list and read the files from the S3 data store.
14. Enable Redshift audit logging in the cluster. Create and configure a new trail in CloudTrail to capture and monitor API calls of Redshift.
- Amazon Redshift provides native audit capabilities that you can enable in the cluster. When enabled, the feature captures SQL queries that are run against the cluster with details including the user, query text, execution plan, timestamp, and performance metrics. This information is stored in query logs that you can analyze for audit purposes. CloudTrail is a service that records API calls made in services that an AWS account uses, including Amazon Redshift. You can configure CloudTrail to capture information related to activities including the creation, modification, or deletion of database objects. Additionally, CloudTrail can capture the identity of the user that performed the API call and timestamp. This solution provides comprehensive audit and traceability of activity in Amazon Redshift with minimal operational effort. This solution uses the native logging capabilities of Amazon Redshift and the Amazon Redshift integration with CloudTrail.
15. DynamicFrame in Glue is used when each record is self-described.
- DynamicFrame is useful for data that does not conform to a fixed schema. Server-side filtering with the catalogPartitionPredicate option takes advantage of partition indexes in Glue Data Catalog. This option improves the processing and response time when scanning heavily partitioned tables. This solution uses the table partition index present in this scenario to improve the ETL performance, which reduces the response time, data scanned, and compute resources.
16. Lake Formation includes features that can manage data lake access controls and apply security to data in the data lake. Lake Formation provides a centralized solution to implement security rules directly into the metadata catalog.
- [Lake Formation](https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html)
17. Pushdown is an optimization technique that you can use to retrieve data closer to the source. Therefore, this solution can reduce the time and the amount of data that is processed. A solution that defines pushdown predicates reduces the number of files that the AWS Glue job reads. This solution will result in reading less data and will help resolve the out-of-memory error.
- [Pushdown in Glue ETL](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-pushdown.html)
- [Out-of-memory](https://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-oom-abnormalities.html)
18. There are two common optimization techniques to optimize query completion times in Athena. One technique is to use columnar formats such as Parquet and ORC. Columnar formats are optimized for distributed analytics workloads.
- The second technique is to use a partitioning strategy aligned with data consumption. The scenario mentioned different locations and a monthly recurrence of the analysis. Therefore, a solution that partitions by location and date would help to optimize the data scanned for each query. This solution would reduce the query completion times and resolve the slow response times in QuickSight dashboards.
19. Set up CloudWatch alarms to monitor and receive notifications of performance issues or failures in the data pipeline.
- [Monitoring with CloudWatch](https://docs.aws.amazon.com/elastictranscoder/latest/developerguide/monitoring-cloudwatch.html)
- [CloudWatch Logs Insights](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html)
20. Use Glue to extract data from the SQL Server database, transform the data by using Spark, and load the data into S3. Use Glue to perform data quality checks and to partition data by date. Use a JDBC driver to connect to SQL Server.
- You can use AWS Glue to perform data quality checks and to partition data. AWS Glue also supports JDBC connections to many database engines.
- Lambda has a maximum run time of 15 minutes. The scenario states that the ETL process can take several hours to run.
- You can use AWS Batch to run ETL jobs on a fleet of EC2 instances that can scale up or down based on data volume and complexity.